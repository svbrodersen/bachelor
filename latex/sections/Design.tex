\section{Design}\label{sec:Design}
\subsection{Requirements}
As described in Section~\ref{sec:computational} the design goal is to implement
a merge sort algorithm running on a bare-metal RISC-V processor. The
implementation should be usable for lists of varying sizes, and should be easily
customisable given different hardware specifications within the RISC-V
ecosystem. This should all be done without the need of an operating system as
described in Section~\ref{sec:bare-metal}.

\subsection{Development platform}
\subsubsection{freeRTOS}
RTOS stands for Real-time operating system. The goal of an RTOS is to provide a
small and simple design, which is easy to port to different architectures.
Furthermore, freeRTOS provides fast execution speeds and methods for
multi-threading, mutexes, semaphores, software timers, and more. FreeRTOS
specifically is a leading open-source RTOS, fitting well with the open-source
idea provided with RISC-V as well. Using an RTOS like freeRTOS would allow for
creating a similar implementation of a parallelized merge sort as provided in
this thesis. It would also facilitate easier portability across various
architectures. However, using freeRTOS would always provide a small overhead
compared to a complete bare-metal implementation and as the aim of this thesis
was to assess the viability of high-performance processors in the data center, a
more custom implementation was instead chosen.

\subsubsection{Unikernel}
A unikernel can be seen as a small footprint single-address space kernel. It
allows a single application to run as if a fully-fledged operating system is in
the background. Thus, it removes many of the obstacles by developing on
bare-metal, as described in Section~\ref{sec:bare-metal}. Generally, these are
used to create specialized images, which bridge the gap between having a
fully-fledged operating system and working directly on bare-metal. However, in
the field of RISC-V, the most prominent seems to be Nanos. Nanos aims to provide
an alternative than the Linux operating system when creating images meant for
virtual machines. This could be cloud computing where something like Docker
might be used together with Linux today. Although unikernels can be created for
specific hardware applications, it seems sparse in the RISC-V development
environment.

\subsubsection{Bare-metal}\label{sec:bare-metal}
Also known as embedded system programming, bare-metal programming involves
developing applications meant to run without an underlying operating system. This
allows for interlinking with specific hardware and enables a more customizable
program tailored to the hardware's specifications. However, this approach comes
with drawbacks, such as the lack of a standard library. Default memory
allocation functions like malloc in C are not implemented on bare-metal systems
because they require an operating system to function. Consequently, developers
must handle memory allocation and deallocation themselves for more advanced
programs. Additionally, standard printing for debugging purposes is left up to
the developer to implement, requiring them to understand hardware
specifications, such as UART printing. \cite{uart}
\footnote{The QEMU virt machine is compatible with UART, redirecting any output
  from stdout
to the terminal instance running the QEMU virt machine by default. In
Section~\ref{sec:validate}, this redirection is used for debugging the
implementation, with output being redirected to a file instead.}

The management of memory often leads to some odd errors when attempting to
debug. Most notably, where one might generally be used to gdb\footnote{gdb is
the GNU Project Debugger later used for debugging in Section~\ref{sec:testing}.}
to detect, when we have reached a stack overflow that is not possible when
working on bare-metal. It is the operating system, which usually tells a
user-level program, that it is outside of its allocated memory space, but as we
have no operating system unless we ourselves implement some error checking, then
there is none. Instead errors show up as variables getting updated without
intentionally meaning too.

However, with the benefits of customization, bare-metal was chosen as the
development platform.

\subsection{Single vs Multicore}\label{sec:singlevsmulti}
The merge sort algorithm can be executed both with serial and parallel
computing. Merge sort, in itself, has a recursive pattern of dividing the list
into subsections and sorting each subsection for itself. This, combined with
parallelized algorithms generally showing better performance than their serial
counterpart, led to the choice of a multicore development for the implementation
in this thesis. \cite{comp_parallel} This decision provides new challenges not
present in serial computing, as the goal is to access the viability of
custom sorting algorithms on bare-metal.

\subsubsection*{Working on multiple cores}
The first method for implementing a multicore merge sort involved using
threading and a scheduler. When we split a given list into two halves, we would
create a thread assigned to sort each sub list. These two lists would be added
to the queue of available threads, after which the job of merging the two lists
could be added to the back of the queue. The merge job would have to check
whether the two sub lists have finished being sorted, but otherwise, assuming a
round-robin scheduler, it would automatically allow for the correct ordering for
the parallelized merge sort algorithm. However, this approach introduces
multiple race conditions; the primary one being implementing a queue capable of
handling concurrent access. The simplest method would be to implement a lock,
allowing mutual exclusion when adding to and removing from the queue. The
downside of implementing a locking queue is that synchronization can lead to
performance issues. Another method would be to implement a lock-free queue,
which should remove any synchronization issues and be a viable solution.
However, I ran into problems with a child thread (created to sort a sub list)
notifying its parent when it has finished sorting the sub list. Each core would
need some way of keeping track of the current thread running and telling the
parent thread (whose job is to sort the child's list and another sub list) when
it has finished. Although this approach is possible, it was scrapped for the
following design due to these challenges.

To simplify initialization, a single core would have the job of splitting the
initial array into sub lists until every core has a single sub list to work on.
While doing the splitting, it would also create threads which have the job of
merging the sub lists once they are finished. This approach would remove the
need for a queue and scheduler in the first place, as each core would, through
its own core ID, know what thread it would have to complete. The issue of
communication with the parent merge would still exist, but the same index used
to find the thread initially could be used to find the parent as well. The
implementation of this approach can be seen in the Implementation section.


\subsection{Context switching}\label{sec:context_switch}
Context switching is an integral part of multithreading. It is the act of
storing the state of the process so that it can later be restored and resume
execution at a later point. Not only can one save the state of the current
process, one can also modify the context such that instead of continuing at the
point of initialization, it instead continues execution at a target function.
Modifying specific registers would also allow for preset values to be loaded as
function parameters. This is done by saving the values of the registers, such
that they all can be restored at a later point to continue execution.

When creating the thread structure, as mentioned previously, it would then be
possible to create context for computing both the merge sort and merge for a
given section of a sublist.


\subsection{Memory}
\subsubsection*{Getting system information}\label{sec:sys-info}
To properly use the memory, we need some information about the system we are
working on. As this thesis is created on a QEMU system, we are able to get the
system information by running the following:
\begin{lstlisting}
qemu-system-riscv32 -machine virt \
-machine dumpdtb=riscv32.dtb
\end{lstlisting}
 This creates a Device Tree Blob (dtb) data file, which contains information
 about the virt qemu-system-riscv32 virtual machine. This format is not usable
 by us at the moment, but by using the Device Tree Compiler (dtc) package, we
 can convert it from the binary dtb format to a human-readable dts format.
\begin{lstlisting}
sudo apt install dtc
dtc -I dtb -O dts -o riscv32.dts riscv32.dtb
\end{lstlisting}
Opening the file up in your favorite text editor, you should see a lot of
information regarding the qemu-system-riscv32 virtual machine. First, note that
the Devicetree specification states that the memory node describes the physical
memory layout for the system. As we want the programs stack to live within the
memory section, this is the section we should find information about starting
address and length of the memory section. The memory node has two required
sections: first, the device\_type, which must simply be 'memory,' and secondly,
the reg value. The reg value consists of an arbitrary number of address and size
pairs that specify the physical address and size of the memory ranges
\cite{DTS}. Furthermore, it is stated that the property name reg has the value
encoded as a number of <u32> cells required to specify the address and length,
which are bus- specific and are specified by the \#address-cells and
\#size-cells properties in the parent of the device node. Looking through our
riscv32.dts file, we find the relevant information to be:

\begin{lstlisting}
#address-cells = <0x02>;
#size-cells = <0x02>;

memory@80000000 {
  device_type = "memory";
  reg = <0x00 0x80000000 0x00 0x8000000>
};
\end{lstlisting}
With the information previously provided, we know that the starting address of
the memory section is at address $0x00 + 0x80000000 = 0x80000000$ and has a size
of $0x00 + 0x8000000$ bytes, which is equivalent to 128MB. To allow space for
saving static values such as .bss and .data sections

\subsubsection*{Memory Layout}
\begin{figure}
  \centering
  {
  \begin{bytefield}{24}
  \begin{rightwordgroup}{RAM Memory}
    \memsection{0x88000000}{0x87FFF800}{4}{mhartid 0 STACK} \\
    \memsection{}{0x87FFF000}{4}{mhartid 1 STACK} \\
    \memsection{}{}{4}{$\downarrow$}\\
    \memsection{}{}{4}{$\uparrow$}\\
    \memsection{$\_stack\_start$ \\ $+ 2*0x400$}{}{4}{thread 1 STACK}\\
    \memsection{$\_stack\_start$ \\ $+ 0x400$}{\_stack\_start}{4}{thread 0 STACK} \\
    \memsection{}{}{2}{.stack section} \\
    \memsection{}{}{2}{.bss section} \\
    \memsection{}{}{2}{.data section}
  \end{rightwordgroup}\\
  \begin{rightwordgroup}{ROM Memory}
    \memsection{0x80100000}{}{2}{.rodata section}\\
    \memsection{}{0x80000000}{2}{.text section}
  \end{rightwordgroup}
  \end{bytefield}
}
  \caption{Memory layout of the QEMU virt machine with a core stack size of 2048
  bytes and a thread stack size of 1024}\label{fig:mem_layout}
\end{figure}
As mentioned, all created threads need to have a separate stack for context
switching to work. Thus, when creating a thread, we have to allocate some
location in RAM to the task the thread has to perform. In
Figure~\ref{fig:mem_layout}, this memory area is denoted with the "thread x
STACK" area. As a design choice, I chose to separate the thread stacks in the
opposite end of RAM from where the core stacks would be allocated. That way, if
I ran into a thread stack overflow, I would know it was caused by the threads
themselves and vice versa with the core stacks. Different sizes of thread stacks
have not been tested, but a size of 1024 seemed to work without issues on
relatively small lists.

At the end of the RAM section is where the individual core stacks would be
allocated. Again, the specific size has not been tested, but with 8 cores and
the chosen stack size of 2048 bytes, assuming a thread stack size of 1024
bytes, we would be able to have approximately 130.032 individual thread stacks
without the two different stack areas overlapping.\footnote{$0x8800000 -
8*0x800 = 0x87ffc000$ would be the end of core stacks. $0x87ffc000 - x*0x400 =
0x80100000 \implies x = 130.032$. However, as the .data, .bss and .text
sections might be saved in the RAM area by the linker script, we don't know the
definitive value of \_stack\_size until the program is fully compiled.}

\section{Design}\label{sec:Design}
\subsection{Requirements}
As described in Section~\ref{sec:problem} the design goal is to implement
a merge sort algorithm running on a bare-metal RISC-V processor. The
implementation should be usable for lists of varying sizes, and should be easily
customisable given different hardware specifications within the RISC-V
ecosystem. This should all be done without the need of an operating system as
described in Section~\ref{sec:bare-metal}.

\subsection{Development platform}\label{sec:bare-metal}
\subsubsection*{freeRTOS}
RTOS stands for real-time operating system. The goal of an RTOS is to provide a
small and simple design, which is easy to port to different architectures.
FreeRTOS provides fast execution speeds and methods for
multi-threading, mutexes, semaphores, software timers, and more. FreeRTOS
specifically is a leading open-source RTOS, fitting well with the open-source
idea provided with RISC-V as well. Using an RTOS like freeRTOS would allow for
creating a similar implementation of a parallelized merge sort as provided in
this thesis. It would also facilitate easier portability across various
architectures. However, using freeRTOS would always provide a small overhead
compared to a complete bare-metal implementation and as a more custom solution
meant for a specific architecture was the development goal, many of the benefits
of freeRTOS seemed less enticing leading to a more custom approach for this
thesis.

\subsubsection*{Unikernel}
A unikernel can be seen as a small footprint single-address space kernel. It
allows a single application to run as if a fully-fledged operating system is in
the background. Thus, it removes many of the obstacles by developing on
bare-metal, as described in Section~\ref{sec:bare-metal}. Generally, these are
used to create specialized images, which bridge the gap between having a
fully-fledged operating system and working directly on bare-metal. This
generally provides better performance, while still reducing the amount of issues
provided by working on bare-metal. However, in the field of RISC-V, the most
prominent seems to be Nanos. Nanos aims to provide an alternative than the Linux
operating system when creating images meant for virtual machines. This could be
cloud computing where something like Docker might be used together with Linux
today. Although unikernels can be created for specific hardware applications, it
seems sparse in the RISC-V development environment and thus not useable for the
work within this thesis.

\subsubsection*{Bare-metal}
Also known as embedded system programming, bare-metal programming involves
developing applications meant to run without an underlying operating system.
This allows for interlinking with specific hardware and enables a more
customizable program tailored to the hardware's specifications. However, this
approach comes with drawbacks, such as the lack of a standard library. Default
memory allocation functions like malloc in C are not implemented on bare-metal
systems because they require an operating system to function. Consequently,
developers must handle memory allocation and deallocation themselves for more
advanced programs. Additionally, standard printing for debugging purposes is
left up to the developer to implement, requiring them to understand hardware
specifications, such as UART printing.\footnote{The QEMU virt machine is
  compatible with UART, redirecting any output from stdout to the terminal
instance running the QEMU virt machine by default. In
Section~\ref{sec:validate}, this redirection is used for debugging the
implementation, with output being redirected to a file instead.}
\cite{uart}

Memory management often leads to unusual errors during debugging. Notably, while
gdb\footnote{gdb is the GNU Project Debugger later used for debugging in
Section~\ref{sec:testing}.} typically detects stack overflows as segmentation
faults, this is not possible on bare-metal. In a typical environment, the
operating system alerts a user-level program when it exceeds its allocated
memory. Without an operating system, unless I implement error checking myself,
no such alerts occur. Instead, a stack overflow will change the values in
different sections without further notice. However, with the benefits of
customization, bare-metal was chosen as the development platform in this thesis.

\subsection{Single vs Multicore}\label{sec:singlevsmulti}
The merge sort algorithm can be executed both with serial and parallel
computing. Merge sort, in itself, has a recursive pattern of dividing the list
into subsections and sorting each subsection for itself. Given that parallel
algorithms often outperform their serial counterparts, this thesis chose a
multicore approach for implementation. \cite{comp_parallel} This decision
introduces new challenges not present in serial computing ranging from race
conditions to memory management of multiple cores and threads.

\subsubsection*{Working on multiple cores}
The first method for implementing a multicore merge sort involved using
threading and a scheduler. When splitting a given list into two halves, I would
create a thread assigned to sort each half. These two halves would be added
to a global queue of available threads, after which the job of merging the two
halves
could be added to the back of the queue. The merge job would have to check
whether the two child threads have finished sorting each half, but otherwise, assuming a
round-robin scheduler, the queue would automatically guarantee correct ordering for
the parallelized merge sort algorithm. However, this approach introduces
multiple race conditions; the primary one being implementing a queue capable of
handling concurrent access. The simplest method would be to implement a lock,
allowing mutual exclusion when adding to and removing from the queue. The
downside of implementing a locking queue is that synchronization means that a
core would have to do unnecessary work while waiting on another core. Another
method would be to implement a lock-free queue, which should remove any
synchronization issues and be a viable solution. Usually such a queue could be
implemented with atomic instructions such as the Compare and Set(CAS) operation
in the x86 ISA. In RISC-V such an instruction is not present, and instead the
operations Load Reserve(lr) and Save Conditional(sc) are present. The lr and sc
operations could be substitutes for the CAS instruction, but in such a case they
would still have to be wrapped within a loop to guarantee that two threads do
not access the same element. Generally, this approach could theoretically
produce a clean solution with high performance where it is lock-free in the
sense that some core will always make progress. However, there must still be
some sort of synchronization between the cores, and thus was not the chosen
method for this thesis.

Instead, the entire thread structure would be initialized before any core begins
running computations on a thread job. To simplify initialization, a single core
would have the task of splitting the initial array into sublists until every
core has a single sublist to work on. While doing the splitting, it would also
create threads responsible for merging the sublists once they are finished. This
approach eliminates the need for a queue and scheduler since each core, through
its own core ID, knows what thread it must complete.


\subsection{Context switching}\label{sec:context_switch}
Context switching is an integral part of multithreading. It is the act of
storing the state of the process so that it can later be restored and resume
execution at a later point. Furthermore, it is possible to modify context, such
that execution can start at another point in the program. With this I can create
the context needed for sorting a specific part of the list, save it in a thread
structure and then a separate core can continue its execution at a later point.


\subsection{Memory}
\subsubsection*{Getting system information}\label{sec:sys-info}
To properly use the memory, I need some information about the system I am
working on. As this thesis is created on a QEMU system, I am able to get the
system information by running the following:
\begin{lstlisting}
qemu-system-riscv32 -machine virt \
-machine dumpdtb=riscv32.dtb
\end{lstlisting}
 This creates a Device Tree Blob (dtb) data file, which contains information
 about the virt qemu-system-riscv32 virtual machine. This format is generally
 not seen as human-readable, but by using the Device Tree Compiler (dtc)
 package, I can convert it from the binary dtb format to a human-readable dts
 format.
\begin{lstlisting}
sudo apt install dtc
dtc -I dtb -O dts -o riscv32.dts riscv32.dtb
\end{lstlisting}

Opening the file in a text editor, you will see a lot of information regarding
the qemu-system-riscv32 virtual machine. First, note that the Devicetree
specification\cite{DTS} states that the memory node describes the physical
memory layout for the system. The memory node has two required sections: first,
the device\_type, which must simply be 'memory,' and secondly, the reg value. The
reg value consists of an arbitrary number of address and size pairs that specify
the physical address and size of the memory ranges \cite{DTS}. Furthermore, it
is stated that the property name reg has the value encoded as a number of <u32>
cells required to specify the address and length, which are bus-specific and are
specified by the \#address-cells and \#size-cells properties in the parent of the
device node. Looking through our riscv32.dts file, I find the relevant
information to be:

\begin{lstlisting}
#address-cells = <0x02>;
#size-cells = <0x02>;

memory@80000000 {
  device_type = "memory";
  reg = <0x00 0x80000000 0x00 0x8000000>
};
\end{lstlisting}
With the information previously provided, I know that the starting address of
the memory section is at address $0x00 + 0x80000000 = 0x80000000$ and has a size
of $0x00 + 0x8000000$ bytes, which is equivalent to 128MB.

\subsubsection*{Memory Layout}
\begin{figure}
  \centering
  {
  \begin{bytefield}{24}
  \begin{rightwordgroup}{RAM Memory}
    \memsection{0x88000000}{0x87FFF800}{4}{mhartid 0 STACK} \\
    \memsection{}{0x87FFF000}{4}{mhartid 1 STACK} \\
    \memsection{}{}{4}{$\downarrow$}\\
    \memsection{}{}{4}{$\uparrow$}\\
    \memsection{$\_stack\_start$ \\ $+ 2*0x400$}{}{4}{thread 1 STACK}\\
    \memsection{$\_stack\_start$ \\ $+ 0x400$}{\_stack\_start}{4}{thread 0 STACK} \\
    \memsection{}{}{2}{.stack section} \\
    \memsection{}{}{2}{.bss section} \\
    \memsection{}{}{2}{.data section}
  \end{rightwordgroup}\\
  \begin{rightwordgroup}{ROM Memory}
    \memsection{0x80100000}{}{2}{.rodata section}\\
    \memsection{}{0x80000000}{2}{.text section}
  \end{rightwordgroup}
  \end{bytefield}
}
  \caption{Memory layout of the QEMU virt machine with a core stack size of 2048
  bytes and a thread stack size of 1024}\label{fig:mem_layout}
\end{figure}
As mentioned, all created threads need to have a separate stack for context
switching to work. Thus, when creating a thread, I have to allocate some
location in RAM to the task the thread has to perform. In
Figure~\ref{fig:mem_layout}, this memory area is denoted with the "thread x
STACK" area. As a design choice, I chose to separate the thread stacks in the
opposite end of RAM from where the core stacks would be allocated. That way, if
I ran into a thread stack overflow it would be while servicing a thread job any
irregularities would show and vice versa for the core stacks. Generally it is a
good custom that the stack size is some multiple of 4 and with some testing 1024
bytes seemed to be large enough for general usage. At the end of the RAM section
is where the individual core stacks would be allocated. Again through some
general testing a size of 2048 bytes generally worked for small lists. It is
important to note, that these values could quickly become too small, and in
Section~\ref{sec:testing} this will be further investigated.

